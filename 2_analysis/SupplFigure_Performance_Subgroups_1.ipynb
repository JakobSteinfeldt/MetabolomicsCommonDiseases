{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lifelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init(num_cpus=30, dashboard_port=24763, dashboard_host=\"0.0.0.0\", include_dashboard=True)#, webui_url=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=40, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "cluster.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = !hostname\n",
    "if \"sc\" in node[0]:\n",
    "    base_path = \"/sc-projects/sc-proj-ukb-cvd\"\n",
    "else: base_path = \"/data/analysis/ag-reils/ag-reils-shared/cardioRS\"\n",
    "print(base_path)\n",
    "\n",
    "project_name = \"210714_metabolomics\"\n",
    "#data_path = \"/data/analysis/ag-reils/steinfej\"\n",
    "data_pre = f\"{base_path}/data/2_datasets_pre/{project_name}\"\n",
    "data_post = f\"{base_path}/data/3_datasets_post/{project_name}\"\n",
    "\n",
    "project_label = \"21_metabolomics_multitask\"\n",
    "project_path = f\"{base_path}/results/projects/{project_label}\"\n",
    "figures_path = f\"{project_path}/figures\"\n",
    "data_results_path = f\"{project_path}/data\"\n",
    "pathlib.Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(data_results_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esclude heart failure, venous thrombosis, aortic anyeurism partition21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"220126\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  pd.read_feather(f\"{data_post}/data_merged.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_models = pd.read_feather(f\"{data_results_path}/predictions_{run}_metabolomics.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = preds_models.endpoint.unique().tolist()\n",
    "partitions = preds_models.partition.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = pd.read_feather(f\"{data_post}/data_merged.feather\")\n",
    "eids_dict = {}\n",
    "for endpoint in tqdm(endpoints):\n",
    "    eids_incl = data_temp.query(f\"NMR_FLAG==True&{endpoint}==False\").eid.to_list()\n",
    "    if endpoint == \"M_MACE\": eids_incl = data_temp.copy().query(f\"NMR_FLAG==True&{endpoint}==False&statins==False\").eid.to_list()\n",
    "    elif endpoint == \"M_breast_cancer\": eids_incl = data_temp.copy().query(f\"NMR_FLAG==True&{endpoint}==False&sex=='Female'\").eid.to_list()\n",
    "    elif endpoint == \"M_ovarian_cancer\": eids_incl = data_temp.copy().query(f\"NMR_FLAG==True&{endpoint}==False&sex=='Female'\").eid.to_list()\n",
    "    elif endpoint == \"M_uterus_cancer\": eids_incl = data_temp.copy().query(f\"NMR_FLAG==True&{endpoint}==False&sex=='Female'\").eid.to_list()\n",
    "    elif endpoint == \"M_prostate_cancer\": eids_incl = data_temp.copy().query(f\"NMR_FLAG==True&{endpoint}==False&sex=='Male'\").eid.to_list()\n",
    "    print(endpoint, len(eids_incl))\n",
    "    eids_dict[endpoint] = eids_incl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds_models.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = preds.endpoint.unique().tolist()\n",
    "endpoint_labels = sorted([f\"{e}_event\" for e in endpoints]+[f\"{e}_event_time\" for e in endpoints])\n",
    "endpoint_data =  pd.read_feather(f\"{data_post}/data_merged.feather\", columns=[\"eid\"] + endpoint_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.partition.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = preds[['eid','endpoint', 'module','features','partition','Ft_10']]#.query(\"split=='test'\")\n",
    "data_test\n",
    "\n",
    "modules = data_test.module.unique().tolist()\n",
    "features = data_test.features.unique().tolist()\n",
    "partitions = data_test.partition.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=[i for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nmr = data.query(\"NMR_FLAG==True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nmr.value_counts(\"ethnic_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_bins(age):\n",
    "    if age < 50: return \"<50\"\n",
    "    if age>=50:\n",
    "        if age<=60: return \"50-60\"\n",
    "        if age>60: return \">60\" \n",
    "data_nmr[\"age\"] = data_nmr[\"age_at_recruitment\"].apply(age_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eids_dict_sg = {c: data_nmr.groupby(c)[\"eid\"].apply(list).to_dict() for c in [\"age\", \"sex\", \"ethnic_background\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, subgroups in eids_dict_sg.items():\n",
    "    print(group)\n",
    "    for subgroup, eids in subgroups.items():\n",
    "        print(subgroup, len(eids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "#from sksurv.metrics import concordance_index_ipcw, brier_score, cumulative_dynamic_auc, integrated_brier_score\n",
    "from lifelines.utils import concordance_index\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "def calculate_per_endpoint(df, endpoint, module, feature, group, subgroup, len_sg, iteration, time):  \n",
    "    event = [0 if (endpoint_event == 0) | (endpoint_event_time > time) else 1 \n",
    "             for endpoint_event, endpoint_event_time in zip(df[endpoint+\"_event\"], df[endpoint+\"_event_time\"])]\n",
    "    event_time = [time if (endpoint_event == 0) | (endpoint_event_time > time) else endpoint_event_time \n",
    "                  for endpoint_event, endpoint_event_time in zip(df[endpoint+\"_event\"], df[endpoint+\"_event_time\"])]\n",
    "    df = df.assign(event = event, event_time = event_time)\n",
    "    df = df.dropna(subset=[\"event_time\", f\"Ft_{time}\", \"event\"], axis=0)\n",
    "    \n",
    "    try:\n",
    "        cindex = 1-concordance_index(df[\"event_time\"], df[f\"Ft_{time}\"], df[\"event\"])\n",
    "    except: cindex=np.nan\n",
    "    return {\"endpoint\":endpoint, \"module\": module, \"features\": feature, \"group\": group, \"subgroup\": subgroup, \"len_sg\": len_sg, \"iteration\": iteration, \"time\":time, \"cindex\":cindex}\n",
    "\n",
    "@ray.remote\n",
    "def calc_per_iteration(data_bm, eids_bs, endpoint, modules, features, group, subgroup, len_sg, iteration):\n",
    "    rows = []\n",
    "    print(group, subgroup)\n",
    "    for module in tqdm(modules, desc=f\"{endpoint} ({iteration})\"): \n",
    "            temp_module = data_bm.query(\"module==@module\")\n",
    "            for feature in features:\n",
    "                temp_features = temp_module.query(\"features==@feature\")\n",
    "                if len(temp_features)>0:\n",
    "                    data_object = temp_features[[\"eid\", \"Ft_10\", f\"{endpoint}_event\", f\"{endpoint}_event_time\"]].set_index(\"eid\").loc[eids_bs].reset_index()\n",
    "                    rows.append(calculate_per_endpoint(data_object, endpoint, module, feature, group, subgroup, len_sg, iteration, time=10))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for endpoint in tqdm(endpoints):\n",
    "    data_bm = data_test.set_index(\"eid\").query(\"endpoint==@endpoint\").merge(\n",
    "        endpoint_data[[\"eid\", f\"{endpoint}_event\", f\"{endpoint}_event_time\"]].set_index(\"eid\"), \n",
    "        left_index=True, right_index=True, how=\"left\").reset_index()\n",
    "    for group, subgroups in eids_dict_sg.items():\n",
    "        for subgroup, eids_sg in subgroups.items():\n",
    "            data_sg = data_bm.query(\"eid==@eids_sg\")\n",
    "            data_sg_id = ray.put(data_sg)\n",
    "            eids_sg = data_sg.eid.unique()\n",
    "            for iteration in iterations: \n",
    "                try:\n",
    "                    eids_bs = np.random.choice(eids_sg, size=len(eids))\n",
    "                    rows.extend([calc_per_iteration.remote(data_sg_id, eids_bs, endpoint, modules, features, group, subgroup, len(eids_sg), iteration)])\n",
    "                except:\n",
    "                    print(endpoint, group, subgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_finished = [y for x in ray.get(rows) for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_endpoints_pp = pd.DataFrame({}).append(rows_finished, ignore_index=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=\"220128\"\n",
    "name = f\"benchmark1000_cindex_subgroups_{run}\"\n",
    "benchmark_endpoints_pp.to_feather(f\"{data_results_path}/{name}.feather\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:riskiano_py39]",
   "language": "python",
   "name": "conda-env-riskiano_py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
